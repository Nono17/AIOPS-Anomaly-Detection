,LineId,YMD,Time,Type,Component,Content
0,1,2021-03-29,"23:58:12,754",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,STARTUP_MSG:
1,2,2021-03-29,"23:58:12,857",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"registered UNIX signal handlers for [TERM, HUP, INT]"
2,3,2021-03-29,"23:58:20,188",INFO,org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker,Scheduling a check for [DISK]file:/home/hadoop3/hadoop/tmp/dfs/data
3,4,2021-03-29,"23:58:21,408",INFO,org.apache.hadoop.metrics2.impl.MetricsConfig,Loaded properties from hadoop-metrics2.properties
4,5,2021-03-29,"23:58:22,527",INFO,org.apache.hadoop.metrics2.impl.MetricsSystemImpl,Scheduled Metric snapshot period at 10 second(s).
5,6,2021-03-29,"23:58:22,527",INFO,org.apache.hadoop.metrics2.impl.MetricsSystemImpl,DataNode metrics system started
6,7,2021-03-29,"23:58:24,157",INFO,org.apache.hadoop.hdfs.server.common.Util,dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
7,8,2021-03-29,"23:58:24,274",INFO,org.apache.hadoop.hdfs.server.datanode.BlockScanner,Initialized block scanner with targetBytesPerSec 1048576
8,9,2021-03-29,"23:58:24,309",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Configured hostname is hadoop-slave1
9,10,2021-03-29,"23:58:24,311",INFO,org.apache.hadoop.hdfs.server.common.Util,dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
10,11,2021-03-29,"23:58:24,363",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Starting DataNode with maxLockedMemory = 0
11,12,2021-03-29,"23:58:24,826",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Opened streaming server at /0.0.0.0:9866
12,13,2021-03-29,"23:58:25,182",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Balancing bandwidth is 10485760 bytes/s
13,14,2021-03-29,"23:58:25,183",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Number threads for balancing is 50
14,15,2021-03-29,"23:58:27,518",INFO,org.eclipse.jetty.util.log,Logging initialized @39374ms
15,16,2021-03-29,"23:58:28,958",INFO,org.apache.hadoop.security.authentication.server.AuthenticationFilter,"Unable to initialize FileSignerSecretProvider, falling back to use random secrets."
16,17,2021-03-29,"23:58:28,989",INFO,org.apache.hadoop.http.HttpRequestLog,Http request log for http.requests.datanode is not defined
17,18,2021-03-29,"23:58:29,069",INFO,org.apache.hadoop.http.HttpServer2,Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
18,19,2021-03-29,"23:58:29,113",INFO,org.apache.hadoop.http.HttpServer2,Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
19,20,2021-03-29,"23:58:29,113",INFO,org.apache.hadoop.http.HttpServer2,Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
20,21,2021-03-29,"23:58:29,113",INFO,org.apache.hadoop.http.HttpServer2,Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
21,22,2021-03-29,"23:58:29,585",INFO,org.apache.hadoop.http.HttpServer2,Jetty bound to port 40017
22,23,2021-03-29,"23:58:29,604",INFO,org.eclipse.jetty.server.Server,"jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827"
23,24,2021-03-29,"23:58:30,540",INFO,org.eclipse.jetty.server.handler.ContextHandler,"Started o.e.j.s.ServletContextHandler@44aeae34{/logs,file:///usr/local/hadoop/logs/,AVAILABLE}"
24,25,2021-03-29,"23:58:30,544",INFO,org.eclipse.jetty.server.handler.ContextHandler,"Started o.e.j.s.ServletContextHandler@40a8a26f{/static,file:///usr/local/hadoop/share/hadoop/hdfs/webapps/static/,AVAILABLE}"
25,26,2021-03-29,"23:58:31,356",INFO,org.eclipse.jetty.server.handler.ContextHandler,"Started o.e.j.w.WebAppContext@702b656a{/,file:///usr/local/hadoop/share/hadoop/hdfs/webapps/datanode/,AVAILABLE}{/datanode}"
26,27,2021-03-29,"23:58:31,455",INFO,org.eclipse.jetty.server.AbstractConnector,"Started ServerConnector@c190cfc{HTTP/1.1,[http/1.1]}{localhost:40017}"
27,28,2021-03-29,"23:58:31,455",INFO,org.eclipse.jetty.server.Server,Started @43311ms
28,29,2021-03-29,"23:58:33,790",INFO,org.apache.hadoop.hdfs.server.datanode.web.DatanodeHttpServer,Listening HTTP traffic on /0.0.0.0:9864
29,30,2021-03-29,"23:58:33,907",INFO,org.apache.hadoop.util.JvmPauseMonitor,Starting JVM pause monitor
30,31,2021-03-29,"23:58:33,909",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,dnUserName = root
31,32,2021-03-29,"23:58:33,910",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,supergroup = supergroup
32,33,2021-03-29,"23:58:34,395",INFO,org.apache.hadoop.ipc.CallQueueManager,"Using callQueue: class java.util.concurrent.LinkedBlockingQueue, queueCapacity: 1000, scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler, ipcBackoff: false."
33,34,2021-03-29,"23:58:34,551",INFO,org.apache.hadoop.ipc.Server,Starting Socket Reader #1 for port 9867
34,35,2021-03-29,"23:58:36,890",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Opened IPC server at /0.0.0.0:9867
35,36,2021-03-29,"23:58:37,120",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Refresh request received for nameservices: null
36,37,2021-03-29,"23:58:37,344",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Starting BPOfferServices for nameservices: <default>
37,38,2021-03-29,"23:58:38,208",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Block pool <registering> (Datanode Uuid unassigned) service to hadoop-master/172.18.0.3:9000 starting to offer service
38,39,2021-03-29,"23:58:38,260",INFO,org.apache.hadoop.ipc.Server,IPC Server Responder: starting
39,40,2021-03-29,"23:58:38,292",INFO,org.apache.hadoop.ipc.Server,IPC Server listener on 9867: starting
40,41,2021-03-29,"23:58:41,025",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
41,42,2021-03-29,"23:58:42,030",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
42,43,2021-03-29,"23:58:43,035",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
43,44,2021-03-29,"23:58:44,037",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
44,45,2021-03-29,"23:58:44,104",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
45,46,2021-03-29,"23:58:44,189",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
46,47,2021-03-29,"23:58:44,204",ERROR,org.apache.hadoop.jmx.JMXJsonServlet,"getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception"
47,48,2021-03-29,"23:58:45,041",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
48,49,2021-03-29,"23:58:46,050",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
49,50,2021-03-29,"23:58:47,061",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
50,51,2021-03-29,"23:58:48,063",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
51,52,2021-03-29,"23:58:49,069",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
52,53,2021-03-29,"23:58:50,071",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 9 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
53,54,2021-03-29,"23:58:50,079",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,Problem connecting to server: hadoop-master/172.18.0.3:9000
54,55,2021-03-29,"23:58:51,336",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
55,56,2021-03-29,"23:58:51,370",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
56,57,2021-03-29,"23:58:51,372",ERROR,org.apache.hadoop.jmx.JMXJsonServlet,"getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception"
57,58,2021-03-29,"23:58:56,085",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
58,59,2021-03-29,"23:58:57,090",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 1 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
59,60,2021-03-29,"23:58:58,091",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 2 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
60,61,2021-03-29,"23:58:59,094",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 3 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
61,62,2021-03-29,"23:58:59,335",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
62,63,2021-03-29,"23:58:59,356",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
63,64,2021-03-29,"23:58:59,358",ERROR,org.apache.hadoop.jmx.JMXJsonServlet,"getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception"
64,65,2021-03-29,"23:59:00,101",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 4 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
65,66,2021-03-29,"23:59:01,103",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 5 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
66,67,2021-03-29,"23:59:02,106",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 6 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
67,68,2021-03-29,"23:59:03,108",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 7 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
68,69,2021-03-29,"23:59:04,109",INFO,org.apache.hadoop.ipc.Client,"Retrying connect to server: hadoop-master/172.18.0.3:9000. Already tried 8 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=10, sleepTime=1000 MILLISECONDS)"
69,70,2021-03-29,"23:59:05,514",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
70,71,2021-03-29,"23:59:05,521",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
71,72,2021-03-29,"23:59:05,522",ERROR,org.apache.hadoop.jmx.JMXJsonServlet,"getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception"
72,73,2021-03-29,"23:59:11,719",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
73,74,2021-03-29,"23:59:11,733",WARN,org.apache.hadoop.hdfs.server.datanode.DataNode,"Block pool ID needed, but service not yet registered with NN, trace:"
74,75,2021-03-29,"23:59:11,741",ERROR,org.apache.hadoop.jmx.JMXJsonServlet,"getting attribute VolumeInfo of Hadoop:service=DataNode,name=DataNodeInfo threw an exception"
75,76,2021-03-29,"23:59:12,392",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to hadoop-master/172.18.0.3:9000
76,77,2021-03-29,"23:59:12,472",INFO,org.apache.hadoop.hdfs.server.common.Storage,"Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)"
77,78,2021-03-29,"23:59:12,715",INFO,org.apache.hadoop.hdfs.server.common.Storage,Lock on /home/hadoop3/hadoop/tmp/dfs/data/in_use.lock acquired by nodename 98@hadoop-slave1
78,79,2021-03-29,"23:59:12,717",INFO,org.apache.hadoop.hdfs.server.common.Storage,Storage directory with location [DISK]file:/home/hadoop3/hadoop/tmp/dfs/data is not formatted for namespace 1745262111. Formatting...
79,80,2021-03-29,"23:59:12,718",INFO,org.apache.hadoop.hdfs.server.common.Storage,Generated new storageID DS-165019ba-b6c9-444e-bce3-18a32bdc4606 for directory /home/hadoop3/hadoop/tmp/dfs/data
80,81,2021-03-29,"23:59:12,861",INFO,org.apache.hadoop.hdfs.server.common.Storage,Analyzing storage directories for bpid BP-23028314-172.17.0.2-1615261831501
81,82,2021-03-29,"23:59:12,862",INFO,org.apache.hadoop.hdfs.server.common.Storage,Locking is disabled for /home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501
82,83,2021-03-29,"23:59:12,863",INFO,org.apache.hadoop.hdfs.server.common.Storage,Block pool storage directory for location [DISK]file:/home/hadoop3/hadoop/tmp/dfs/data and block pool id BP-23028314-172.17.0.2-1615261831501 is not formatted. Formatting ...
83,84,2021-03-29,"23:59:12,863",INFO,org.apache.hadoop.hdfs.server.common.Storage,Formatting block pool BP-23028314-172.17.0.2-1615261831501 directory /home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current
84,85,2021-03-29,"23:59:12,918",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Setting up storage: nsid=1745262111;bpid=BP-23028314-172.17.0.2-1615261831501;lv=-57;nsInfo=lv=-65;cid=CID-5d2d06ad-4f9e-4a75-bca5-3151ff84f93a;nsid=1745262111;c=1615261831501;bpid=BP-23028314-172.17.0.2-1615261831501;dnuuid=null
85,86,2021-03-29,"23:59:12,963",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Generated and persisted new Datanode UUID bd513feb-d5a8-4136-a028-7eace6b61f98
86,87,2021-03-29,"23:59:14,331",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Added new volume: DS-165019ba-b6c9-444e-bce3-18a32bdc4606
87,88,2021-03-29,"23:59:14,332",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,"Added volume - [DISK]file:/home/hadoop3/hadoop/tmp/dfs/data, StorageType: DISK"
88,89,2021-03-29,"23:59:14,342",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Registered FSDatasetState MBean
89,90,2021-03-29,"23:59:14,351",INFO,org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker,Scheduling a check for /home/hadoop3/hadoop/tmp/dfs/data
90,91,2021-03-29,"23:59:14,387",INFO,org.apache.hadoop.hdfs.server.datanode.checker.DatasetVolumeChecker,Scheduled health check for volume /home/hadoop3/hadoop/tmp/dfs/data
91,92,2021-03-29,"23:59:14,423",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Adding block pool BP-23028314-172.17.0.2-1615261831501
92,93,2021-03-29,"23:59:14,445",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Scanning block pool BP-23028314-172.17.0.2-1615261831501 on volume /home/hadoop3/hadoop/tmp/dfs/data...
93,94,2021-03-29,"23:59:15,024",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Time taken to scan block pool BP-23028314-172.17.0.2-1615261831501 on /home/hadoop3/hadoop/tmp/dfs/data: 568ms
94,95,2021-03-29,"23:59:15,025",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Total time to scan all replicas for block pool BP-23028314-172.17.0.2-1615261831501: 602ms
95,96,2021-03-29,"23:59:15,035",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Adding replicas to map for block pool BP-23028314-172.17.0.2-1615261831501 on volume /home/hadoop3/hadoop/tmp/dfs/data...
96,97,2021-03-29,"23:59:15,036",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.BlockPoolSlice,Replica Cache file: /home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/replicas doesn't exist
97,98,2021-03-29,"23:59:15,048",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Time to add replicas to map for block pool BP-23028314-172.17.0.2-1615261831501 on volume /home/hadoop3/hadoop/tmp/dfs/data: 13ms
98,99,2021-03-29,"23:59:15,049",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetImpl,Total time to add all replicas to map for block pool BP-23028314-172.17.0.2-1615261831501: 17ms
99,100,2021-03-29,"23:59:15,107",INFO,org.apache.hadoop.hdfs.server.datanode.VolumeScanner,Now scanning bpid BP-23028314-172.17.0.2-1615261831501 on volume /home/hadoop3/hadoop/tmp/dfs/data
100,101,2021-03-29,"23:59:15,142",INFO,org.apache.hadoop.hdfs.server.datanode.VolumeScanner,"VolumeScanner(/home/hadoop3/hadoop/tmp/dfs/data, DS-165019ba-b6c9-444e-bce3-18a32bdc4606): finished scanning block pool BP-23028314-172.17.0.2-1615261831501"
101,102,2021-03-29,"23:59:15,324",INFO,org.apache.hadoop.hdfs.server.datanode.VolumeScanner,"VolumeScanner(/home/hadoop3/hadoop/tmp/dfs/data, DS-165019ba-b6c9-444e-bce3-18a32bdc4606): no suitable block pools found to scan.  Waiting 1814399777 ms."
102,103,2021-03-29,"23:59:15,445",INFO,org.apache.hadoop.hdfs.server.datanode.DirectoryScanner,Periodic Directory Tree Verification scan starting at 3/30/21 12:20 AM with interval of 21600000ms
103,104,2021-03-29,"23:59:15,571",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Block pool BP-23028314-172.17.0.2-1615261831501 (Datanode Uuid bd513feb-d5a8-4136-a028-7eace6b61f98) service to hadoop-master/172.18.0.3:9000 beginning handshake with NN
104,105,2021-03-29,"23:59:16,237",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Block pool Block pool BP-23028314-172.17.0.2-1615261831501 (Datanode Uuid bd513feb-d5a8-4136-a028-7eace6b61f98) service to hadoop-master/172.18.0.3:9000 successfully registered with NN
105,106,2021-03-29,"23:59:16,237",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,For namenode hadoop-master/172.18.0.3:9000 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
106,107,2021-03-29,"23:59:18,277",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"Successfully sent block report 0x48d14e57b126fb91,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 25 msec to generate and 538 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5."
107,108,2021-03-29,"23:59:18,278",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Got finalize command for block pool BP-23028314-172.17.0.2-1615261831501
108,109,2021-03-30,"00:00:49,746",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741825_1001 src: /172.18.0.4:52744 dest: /172.18.0.2:9866
109,110,2021-03-30,"00:00:50,155",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.4:52744, dest: /172.18.0.2:9866, bytes: 13, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_523782650_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741825_1001, duration(ns): 133273916"
110,111,2021-03-30,"00:00:50,155",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating"
111,112,2021-03-30,"00:00:50,844",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741826_1002 src: /172.18.0.3:54992 dest: /172.18.0.2:9866
112,113,2021-03-30,"00:00:50,847",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
113,114,2021-03-30,"00:00:50,950",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.3:54992, dest: /172.18.0.2:9866, bytes: 13, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_523782650_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741826_1002, duration(ns): 20651114"
114,115,2021-03-30,"00:00:50,951",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741826_1002, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
115,116,2021-03-30,"00:00:57,297",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741827_1003 src: /172.18.0.4:46052 dest: /172.18.0.2:9866
116,117,2021-03-30,"00:00:57,450",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.4:46052, dest: /172.18.0.2:9866, bytes: 317057, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-562512649_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741827_1003, duration(ns): 138881588"
117,118,2021-03-30,"00:00:57,451",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating"
118,119,2021-03-30,"00:00:57,996",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741828_1004 src: /172.18.0.3:47288 dest: /172.18.0.2:9866
119,120,2021-03-30,"00:00:57,997",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
120,121,2021-03-30,"00:00:58,054",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.3:47288, dest: /172.18.0.2:9866, bytes: 239, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-562512649_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741828_1004, duration(ns): 41870315"
121,122,2021-03-30,"00:00:58,054",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741828_1004, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
122,123,2021-03-30,"00:00:58,619",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741829_1005 src: /172.18.0.3:49212 dest: /172.18.0.2:9866
123,124,2021-03-30,"00:00:58,620",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
124,125,2021-03-30,"00:00:58,687",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.3:49212, dest: /172.18.0.2:9866, bytes: 72, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-562512649_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741829_1005, duration(ns): 50737815"
125,126,2021-03-30,"00:00:58,687",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741829_1005, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
126,127,2021-03-30,"00:00:59,408",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741830_1006 src: /172.18.0.3:50816 dest: /172.18.0.2:9866
127,128,2021-03-30,"00:00:59,409",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
128,129,2021-03-30,"00:00:59,569",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.3:50816, dest: /172.18.0.2:9866, bytes: 192930, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_-562512649_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741830_1006, duration(ns): 120630930"
129,130,2021-03-30,"00:00:59,569",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741830_1006, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
130,131,2021-03-30,"00:01:30,064",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741831_1007 src: /172.18.0.2:60144 dest: /172.18.0.2:9866
131,132,2021-03-30,"00:01:30,070",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
132,133,2021-03-30,"00:01:30,210",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.2:60144, dest: /172.18.0.2:9866, bytes: 223541, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1509079730_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741831_1007, duration(ns): 67986610"
133,134,2021-03-30,"00:01:30,211",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741831_1007, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
134,135,2021-03-30,"00:01:42,645",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741832_1008 src: /172.18.0.2:41352 dest: /172.18.0.2:9866
135,136,2021-03-30,"00:01:42,647",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
136,137,2021-03-30,"00:02:00,487",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741833_1009 src: /172.18.0.4:42830 dest: /172.18.0.2:9866
137,138,2021-03-30,"00:02:00,588",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.4:42830, dest: /172.18.0.2:9866, bytes: 26, op: HDFS_WRITE, cliID: DFSClient_attempt_1617062369629_0001_r_000000_0_-996629036_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741833_1009, duration(ns): 96031120"
138,139,2021-03-30,"00:02:00,588",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating"
139,140,2021-03-30,"00:02:03,466",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.2:41352, dest: /172.18.0.2:9866, bytes: 26621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1509079730_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741832_1008, duration(ns): 20805437040"
140,141,2021-03-30,"00:02:03,466",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741832_1008, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
141,142,2021-03-30,"00:02:03,716",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741834_1010 src: /172.18.0.2:43400 dest: /172.18.0.2:9866
142,143,2021-03-30,"00:02:03,718",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
143,144,2021-03-30,"00:02:03,787",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.2:43400, dest: /172.18.0.2:9866, bytes: 441, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1509079730_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741834_1010, duration(ns): 50685497"
144,145,2021-03-30,"00:02:03,788",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741834_1010, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
145,146,2021-03-30,"00:02:04,597",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741835_1011 src: /172.18.0.2:45674 dest: /172.18.0.2:9866
146,147,2021-03-30,"00:02:04,599",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
147,148,2021-03-30,"00:02:04,741",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.2:45674, dest: /172.18.0.2:9866, bytes: 26621, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1509079730_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741835_1011, duration(ns): 127001716"
148,149,2021-03-30,"00:02:04,742",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741835_1011, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
149,150,2021-03-30,"00:02:05,107",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,Receiving BP-23028314-172.17.0.2-1615261831501:blk_1073741836_1012 src: /172.18.0.2:47144 dest: /172.18.0.2:9866
150,151,2021-03-30,"00:02:05,109",INFO,org.apache.hadoop.hdfs.protocol.datatransfer.sasl.SaslDataTransferClient,"SASL encryption trust check: localHostTrusted = false, remoteHostTrusted = false"
151,152,2021-03-30,"00:02:05,275",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode.clienttrace,"src: /172.18.0.2:47144, dest: /172.18.0.2:9866, bytes: 223541, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1509079730_1, offset: 0, srvID: bd513feb-d5a8-4136-a028-7eace6b61f98, blockid: BP-23028314-172.17.0.2-1615261831501:blk_1073741836_1012, duration(ns): 122911169"
152,153,2021-03-30,"00:02:05,275",INFO,org.apache.hadoop.hdfs.server.datanode.DataNode,"PacketResponder: BP-23028314-172.17.0.2-1615261831501:blk_1073741836_1012, type=HAS_DOWNSTREAM_IN_PIPELINE, downstreams=1:[172.18.0.4:9866] terminating"
153,154,2021-03-30,"00:02:11,325",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,"Scheduling blk_1073741827_1003 replica FinalizedReplica, blk_1073741827_1003, FINALIZED"
154,155,2021-03-30,"00:02:11,349",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,"Scheduling blk_1073741828_1004 replica FinalizedReplica, blk_1073741828_1004, FINALIZED"
155,156,2021-03-30,"00:02:11,349",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,"Scheduling blk_1073741829_1005 replica FinalizedReplica, blk_1073741829_1005, FINALIZED"
156,157,2021-03-30,"00:02:11,350",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,"Scheduling blk_1073741830_1006 replica FinalizedReplica, blk_1073741830_1006, FINALIZED"
157,158,2021-03-30,"00:02:11,350",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,"Scheduling blk_1073741831_1007 replica FinalizedReplica, blk_1073741831_1007, FINALIZED"
158,159,2021-03-30,"00:02:11,350",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,"Scheduling blk_1073741832_1008 replica FinalizedReplica, blk_1073741832_1008, FINALIZED"
159,160,2021-03-30,"00:02:11,360",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,Deleted BP-23028314-172.17.0.2-1615261831501 blk_1073741827_1003 URI file:/home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/finalized/subdir0/subdir0/blk_1073741827
160,161,2021-03-30,"00:02:11,370",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,Deleted BP-23028314-172.17.0.2-1615261831501 blk_1073741828_1004 URI file:/home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/finalized/subdir0/subdir0/blk_1073741828
161,162,2021-03-30,"00:02:11,370",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,Deleted BP-23028314-172.17.0.2-1615261831501 blk_1073741829_1005 URI file:/home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/finalized/subdir0/subdir0/blk_1073741829
162,163,2021-03-30,"00:02:11,371",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,Deleted BP-23028314-172.17.0.2-1615261831501 blk_1073741831_1007 URI file:/home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/finalized/subdir0/subdir0/blk_1073741831
163,164,2021-03-30,"00:02:11,371",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,Deleted BP-23028314-172.17.0.2-1615261831501 blk_1073741830_1006 URI file:/home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/finalized/subdir0/subdir0/blk_1073741830
164,165,2021-03-30,"00:02:11,372",INFO,org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetAsyncDiskService,Deleted BP-23028314-172.17.0.2-1615261831501 blk_1073741832_1008 URI file:/home/hadoop3/hadoop/tmp/dfs/data/current/BP-23028314-172.17.0.2-1615261831501/current/finalized/subdir0/subdir0/blk_1073741832
165,166,2021-03-30,"00:02:35,098",INFO,org.apache.hadoop.util.JvmPauseMonitor,Detected pause in JVM or host machine (eg GC): pause of approximately 4329ms
166,167,GC,pool,'PS,MarkSweep' had collection(s),count=1 time=3651ms
167,168,GC,pool,'PS,Scavenge' had collection(s),count=1 time=631ms
